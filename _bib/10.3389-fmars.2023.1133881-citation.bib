@ARTICLE{10.3389/fmars.2023.1133881,

AUTHOR={Xin, Zhichao  and Wang, Zhe  and Yu, Zhibin  and Zheng, Bing },

TITLE={ULL-SLAM: underwater low-light enhancement for the front-end of visual SLAM},

JOURNAL={Frontiers in Marine Science},

VOLUME={10},

YEAR={2023},

URL={https://www.frontiersin.org/journals/marine-science/articles/10.3389/fmars.2023.1133881},

DOI={10.3389/fmars.2023.1133881},

ISSN={2296-7745},

ABSTRACT={<p>Underwater visual simultaneous localization and mapping (VSLAM), which can provide robot navigation and localization for underwater vehicles, is crucial in underwater exploration. Underwater SLAM is a challenging research topic due to the limitations of underwater vision and error accumulation over long-term operations. When an underwater vehicle goes down, it may inevitably enter a low-light environment. Although artificial light sources could help to some extent, they might also cause non-uniform illumination, which may have an adverse effect on feature point matching. Consequently, the capability of feature point extraction-based visual SLAM systems could only sometimes work. This paper proposes an end-to-end network for SLAM preprocessing in an underwater low-light environment to address this issue. Our model includes a low-light enhancement branch specific with a non-reference loss function, which can achieve low-light image enhancement without requiring paired low-light data. In addition, we design a self-supervised feature point detector and descriptor extraction branch to take advantage of self-supervised learning for feature points and descriptors matching to reduce the re-projection error. Unlike other works, our model does not require pseudo-ground truth. Finally, we design a unique matrix transformation method to improve the feature similarity between two adjacent video frames. Comparative experiments and ablation experiments confirm that the proposed method in this paper could effectively enhance the performance of VSLAM based on feature point extraction in an underwater low-light environment.</p>}}